{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74443269-c41a-44b0-8dd3-4d3c9b7078ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5c5a47-85f0-4f44-aff6-e9e8d52c0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"all_images\"\n",
    "image_list = []\n",
    "label_list = []\n",
    "pfirrman_grades = []\n",
    "modic_types = []\n",
    "df = pd.read_csv('AggregatePTData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2cad52-3618-41b3-9b41-d1d049de2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_filename in os.listdir(image_folder):\n",
    "    if image_filename.endswith(\".png\"):\n",
    "        image = Image.open(os.path.join(image_folder, image_filename))\n",
    "        # Resize image to 224x224 pixels\n",
    "        image = image.resize((224,224))\n",
    "        # Convert image to RGB\n",
    "        image = image.convert(\"RGB\")\n",
    "        # Convert the image to numpy array\n",
    "        image_array = np.array(image)\n",
    "        # Normalize array\n",
    "        image_array = preprocess_input(image_array)\n",
    "        image_list.append(image_array)\n",
    "        \n",
    "        row = df[df['file_name'] == image_filename]\n",
    "        pfirrman_grade = row.iloc[0]['Pfirrman grade']\n",
    "        pfirrman_grades.append(pfirrman_grade)\n",
    "        modic = row.iloc[0]['Modic']\n",
    "        modic_types.append(modic)\n",
    "\n",
    "image_list_np = np.array(image_list)\n",
    "pfirrman_grades_np = np.array(pfirrman_grades)\n",
    "pfirrman_grades_np = np.round(pfirrman_grades_np).astype(int) - 1\n",
    "modic_types_np = np.array(modic_types)\n",
    "modic_types_np = np.round(modic_types_np).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72265a16-e366-41d1-b73b-a28336cf7d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3f80ca-a2bd-4e46-a786-de1214b5c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Global average pooling to reduce dimensions\n",
    "x = Dense(1024, activation='relu')(x)  # Fully connected layer\n",
    "pfirrman_predictions = Dense(5, activation='softmax', name='pfirrman_output')(x)\n",
    "modic_predictions = Dense(3, activation='softmax', name='modic_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1ff725-b21f-4db3-afd2-7bd4f51c9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=[pfirrman_predictions, modic_predictions])\n",
    "model.compile(optimizer='adam', loss={\n",
    "        'pfirrman_output': 'sparse_categorical_crossentropy', \n",
    "        'modic_output': 'sparse_categorical_crossentropy'\n",
    "    },          \n",
    "    metrics={\n",
    "        'pfirrman_output': 'accuracy',\n",
    "        'modic_output': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54efd51-33d1-41b4-a847-a3eafa79c5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c420a1-67bc-4e16-8ec2-b25d71ffd6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - loss: 4.9532 - modic_output_accuracy: 0.5283 - modic_output_loss: 1.7153 - pfirrman_output_accuracy: 0.2160 - pfirrman_output_loss: 3.2379 - val_loss: 3.8203 - val_modic_output_accuracy: 0.4795 - val_modic_output_loss: 1.9256 - val_pfirrman_output_accuracy: 0.3973 - val_pfirrman_output_loss: 1.9222\n",
      "Epoch 2/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - loss: 2.7674 - modic_output_accuracy: 0.5421 - modic_output_loss: 1.1917 - pfirrman_output_accuracy: 0.5010 - pfirrman_output_loss: 1.5757 - val_loss: 2.5535 - val_modic_output_accuracy: 0.4521 - val_modic_output_loss: 1.1062 - val_pfirrman_output_accuracy: 0.3973 - val_pfirrman_output_loss: 1.5164\n",
      "Epoch 3/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 1.6683 - modic_output_accuracy: 0.7097 - modic_output_loss: 0.6806 - pfirrman_output_accuracy: 0.5936 - pfirrman_output_loss: 0.9877 - val_loss: 2.5987 - val_modic_output_accuracy: 0.5342 - val_modic_output_loss: 1.0127 - val_pfirrman_output_accuracy: 0.4384 - val_pfirrman_output_loss: 1.6278\n",
      "Epoch 4/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 1.3042 - modic_output_accuracy: 0.7668 - modic_output_loss: 0.5975 - pfirrman_output_accuracy: 0.7095 - pfirrman_output_loss: 0.7068 - val_loss: 2.5296 - val_modic_output_accuracy: 0.5068 - val_modic_output_loss: 1.0160 - val_pfirrman_output_accuracy: 0.4247 - val_pfirrman_output_loss: 1.5066\n",
      "Epoch 5/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 1.0582 - modic_output_accuracy: 0.8136 - modic_output_loss: 0.4990 - pfirrman_output_accuracy: 0.8465 - pfirrman_output_loss: 0.5592 - val_loss: 2.9619 - val_modic_output_accuracy: 0.4384 - val_modic_output_loss: 1.3051 - val_pfirrman_output_accuracy: 0.4110 - val_pfirrman_output_loss: 1.6130\n",
      "Epoch 6/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 1.0720 - modic_output_accuracy: 0.8152 - modic_output_loss: 0.5148 - pfirrman_output_accuracy: 0.7924 - pfirrman_output_loss: 0.5572 - val_loss: 3.0148 - val_modic_output_accuracy: 0.4521 - val_modic_output_loss: 1.2079 - val_pfirrman_output_accuracy: 0.3562 - val_pfirrman_output_loss: 1.9115\n",
      "Epoch 7/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 0.9038 - modic_output_accuracy: 0.8239 - modic_output_loss: 0.4135 - pfirrman_output_accuracy: 0.8034 - pfirrman_output_loss: 0.4903 - val_loss: 2.7397 - val_modic_output_accuracy: 0.4658 - val_modic_output_loss: 1.0548 - val_pfirrman_output_accuracy: 0.3836 - val_pfirrman_output_loss: 1.7285\n",
      "Epoch 8/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 0.6530 - modic_output_accuracy: 0.9328 - modic_output_loss: 0.2756 - pfirrman_output_accuracy: 0.8676 - pfirrman_output_loss: 0.3774 - val_loss: 2.7434 - val_modic_output_accuracy: 0.5342 - val_modic_output_loss: 0.9993 - val_pfirrman_output_accuracy: 0.4110 - val_pfirrman_output_loss: 1.6651\n",
      "Epoch 9/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 0.5706 - modic_output_accuracy: 0.9723 - modic_output_loss: 0.2346 - pfirrman_output_accuracy: 0.8845 - pfirrman_output_loss: 0.3360 - val_loss: 3.2172 - val_modic_output_accuracy: 0.5616 - val_modic_output_loss: 0.9831 - val_pfirrman_output_accuracy: 0.3699 - val_pfirrman_output_loss: 2.0059\n",
      "Epoch 10/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 0.5297 - modic_output_accuracy: 0.9299 - modic_output_loss: 0.2570 - pfirrman_output_accuracy: 0.9286 - pfirrman_output_loss: 0.2727 - val_loss: 3.1242 - val_modic_output_accuracy: 0.4795 - val_modic_output_loss: 1.0889 - val_pfirrman_output_accuracy: 0.3836 - val_pfirrman_output_loss: 1.8476\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    image_list_np,            # Input images\n",
    "    [pfirrman_grades_np, modic_types_np],\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,     # Split 20% of data for validation\n",
    "    verbose=1                 # Set to 1 to see training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf8af678-130a-4b8a-ad5e-23dc198730b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Predicted Pfirrman grade: 3\n",
      "Predicted Modic: 0\n"
     ]
    }
   ],
   "source": [
    "test_image_path = 'all_images/51_t1.png'\n",
    "\n",
    "# Load, resize, and preprocess the test image\n",
    "img = Image.open(test_image_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "img_array = np.array(img_resized)\n",
    "\n",
    "# Preprocess the image for ResNet50\n",
    "img_preprocessed = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "img_preprocessed = np.expand_dims(img_preprocessed, axis=0)  # Add batch dimension\n",
    "\n",
    "# Run the prediction\n",
    "predicted_probabilities = model.predict(img_preprocessed)\n",
    "predicted_pfirrman = np.argmax(predicted_probabilities[0]) + 1  # Add 1 if classes are 1-5\n",
    "predicted_modic = np.argmax(predicted_probabilities[1])\n",
    "print(f\"Predicted Pfirrman grade: {predicted_pfirrman}\")\n",
    "print(f\"Predicted Modic: {predicted_modic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad646b-7546-4d27-9208-344aaf9add53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "795e59ff-6e75-464b-968d-4f9f24745790",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nonBinaryPredictions.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24579fa5-8221-4aab-b9f5-37d38d93ae5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
