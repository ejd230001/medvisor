{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b363bd-0343-4529-a7a1-42331874856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0f1877-8173-4dea-b4c8-db9c52d8dbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels have been mapped to each image and stored in the corresponding lists.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = 'radiological_gradings.csv'\n",
    "radiological_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Initialize lists for each label\n",
    "pfirrman_grades = []\n",
    "modic_types = []\n",
    "up_endplates = []\n",
    "low_endplates = []\n",
    "disc_herniation = []\n",
    "disc_narrowing = []\n",
    "disc_bulging = []\n",
    "spondylolisthesis = []\n",
    "image_list = []  # For storing image data\n",
    "image_names = []\n",
    "\n",
    "# Directory containing the disc images\n",
    "image_dir = \"split_discs\"\n",
    "\n",
    "# Loop through each image in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith(\".png\"):\n",
    "        # Parse patient number and IVD label from the filename\n",
    "        base_name = filename.replace(\".png\", \"\")\n",
    "        \n",
    "        # Split the filename and handle cases with additional parts\n",
    "        parts = base_name.split(\"_\")\n",
    "        patient_id = int(parts[0])           # First part is always the patient ID\n",
    "        scan_type = parts[-2]                # Second last part is the scan type (t1/t2)\n",
    "        ivd_label = int(parts[-1].replace(\"disc\", \"\"))  # Last part is the disc label\n",
    "        \n",
    "        # Find the corresponding row in the CSV\n",
    "        row = radiological_data[(radiological_data['Patient'] == patient_id) &\n",
    "                                (radiological_data['IVD label'] == ivd_label)]\n",
    "        \n",
    "        # Ensure the row exists and extract label values\n",
    "        if not row.empty:\n",
    "            # Append each label to its respective list\n",
    "            modic_types.append(row['Modic'].values[0])\n",
    "            up_endplates.append(row['UP endplate'].values[0])\n",
    "            low_endplates.append(row['LOW endplate'].values[0])\n",
    "            spondylolisthesis.append(row['Spondylolisthesis'].values[0])\n",
    "            disc_herniation.append(row['Disc herniation'].values[0])\n",
    "            disc_narrowing.append(row['Disc narrowing'].values[0])\n",
    "            disc_bulging.append(row['Disc bulging'].values[0])\n",
    "            pfirrman_grades.append(row['Pfirrman grade'].values[0])\n",
    "            \n",
    "            # Optionally load the image data if needed\n",
    "            image = Image.open(os.path.join(image_dir, filename))\n",
    "            # Resize image to 224x224 pixels\n",
    "            image = image.resize((224,224))\n",
    "            # Convert image to RGB\n",
    "            image = image.convert(\"RGB\")\n",
    "            # Convert the image to numpy array\n",
    "            image_array = np.array(image)\n",
    "            # Normalize array\n",
    "            image_array = preprocess_input(image_array)\n",
    "            image_list.append(image_array)\n",
    "            image_names.append(filename)\n",
    "\n",
    "print(\"Labels have been mapped to each image and stored in the corresponding lists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec4e007-abaf-4f2d-a6ad-11cda9d3be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_np = np.array(image_list)\n",
    "pfirrman_grades_np = np.array(pfirrman_grades)\n",
    "pfirrman_grades_np = np.round(pfirrman_grades_np).astype(int) - 1\n",
    "modic_types_np = np.array(modic_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85134c6a-4ce5-4c6a-88ca-308a82f0eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Global average pooling to reduce dimensions\n",
    "x = Dense(1024, activation='relu')(x)  # Fully connected layer\n",
    "pfirrman_predictions = Dense(5, activation='softmax', name='pfirrman_output')(x)\n",
    "modic_predictions = Dense(4, activation='softmax', name='modic_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7587da4-fe37-469f-80ab-8be939dda1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=[pfirrman_predictions, modic_predictions])\n",
    "model.compile(optimizer='adam', loss={\n",
    "        'pfirrman_output': 'sparse_categorical_crossentropy', \n",
    "        'modic_output': 'sparse_categorical_crossentropy'\n",
    "    },          \n",
    "    metrics={\n",
    "        'pfirrman_output': 'accuracy',\n",
    "        'modic_output': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28a2936-73fb-4e10-9bcb-e5903b4a9b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - loss: 4.0199 - modic_output_accuracy: 0.5682 - modic_output_loss: 2.0582 - pfirrman_output_accuracy: 0.2832 - pfirrman_output_loss: 1.9607 - val_loss: 2.7074 - val_modic_output_accuracy: 0.5234 - val_modic_output_loss: 1.1882 - val_pfirrman_output_accuracy: 0.3458 - val_pfirrman_output_loss: 1.5054\n",
      "Epoch 2/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - loss: 1.8530 - modic_output_accuracy: 0.7155 - modic_output_loss: 0.5593 - pfirrman_output_accuracy: 0.4796 - pfirrman_output_loss: 1.2933 - val_loss: 2.5895 - val_modic_output_accuracy: 0.5093 - val_modic_output_loss: 0.9337 - val_pfirrman_output_accuracy: 0.2710 - val_pfirrman_output_loss: 1.6325\n",
      "Epoch 3/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.5557 - modic_output_accuracy: 0.7724 - modic_output_loss: 0.4756 - pfirrman_output_accuracy: 0.5606 - pfirrman_output_loss: 1.0801 - val_loss: 2.9819 - val_modic_output_accuracy: 0.5187 - val_modic_output_loss: 1.1948 - val_pfirrman_output_accuracy: 0.3131 - val_pfirrman_output_loss: 1.7876\n",
      "Epoch 4/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.5192 - modic_output_accuracy: 0.7610 - modic_output_loss: 0.4938 - pfirrman_output_accuracy: 0.5982 - pfirrman_output_loss: 1.0252 - val_loss: 2.6266 - val_modic_output_accuracy: 0.5000 - val_modic_output_loss: 0.9198 - val_pfirrman_output_accuracy: 0.3551 - val_pfirrman_output_loss: 1.6857\n",
      "Epoch 5/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - loss: 1.2622 - modic_output_accuracy: 0.8638 - modic_output_loss: 0.3980 - pfirrman_output_accuracy: 0.6781 - pfirrman_output_loss: 0.8646 - val_loss: 2.8195 - val_modic_output_accuracy: 0.5280 - val_modic_output_loss: 0.8900 - val_pfirrman_output_accuracy: 0.2570 - val_pfirrman_output_loss: 1.9092\n",
      "Epoch 6/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.2147 - modic_output_accuracy: 0.8281 - modic_output_loss: 0.3690 - pfirrman_output_accuracy: 0.6599 - pfirrman_output_loss: 0.8460 - val_loss: 2.6568 - val_modic_output_accuracy: 0.5000 - val_modic_output_loss: 0.9149 - val_pfirrman_output_accuracy: 0.2944 - val_pfirrman_output_loss: 1.7219\n",
      "Epoch 7/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - loss: 1.0685 - modic_output_accuracy: 0.8493 - modic_output_loss: 0.3572 - pfirrman_output_accuracy: 0.7358 - pfirrman_output_loss: 0.7114 - val_loss: 2.9519 - val_modic_output_accuracy: 0.4813 - val_modic_output_loss: 1.0611 - val_pfirrman_output_accuracy: 0.2617 - val_pfirrman_output_loss: 1.8710\n",
      "Epoch 8/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.0069 - modic_output_accuracy: 0.8653 - modic_output_loss: 0.3161 - pfirrman_output_accuracy: 0.7258 - pfirrman_output_loss: 0.6910 - val_loss: 3.0120 - val_modic_output_accuracy: 0.5047 - val_modic_output_loss: 0.9778 - val_pfirrman_output_accuracy: 0.2664 - val_pfirrman_output_loss: 2.0032\n",
      "Epoch 9/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 0.8815 - modic_output_accuracy: 0.8803 - modic_output_loss: 0.3045 - pfirrman_output_accuracy: 0.7878 - pfirrman_output_loss: 0.5769 - val_loss: 3.3064 - val_modic_output_accuracy: 0.5140 - val_modic_output_loss: 1.3313 - val_pfirrman_output_accuracy: 0.3598 - val_pfirrman_output_loss: 1.9520\n",
      "Epoch 10/10\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 0.8472 - modic_output_accuracy: 0.8874 - modic_output_loss: 0.2722 - pfirrman_output_accuracy: 0.8048 - pfirrman_output_loss: 0.5749 - val_loss: 3.2063 - val_modic_output_accuracy: 0.5000 - val_modic_output_loss: 1.1787 - val_pfirrman_output_accuracy: 0.3037 - val_pfirrman_output_loss: 2.0071\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    image_list_np,            # Input images\n",
    "    [pfirrman_grades_np, modic_types_np],\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,     # Split 20% of data for validation\n",
    "    verbose=1                 # Set to 1 to see training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f30bc1-a837-4560-b80e-ed94ba561b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Predicted Pfirrman grade: 1\n",
      "Predicted Modic: 0\n"
     ]
    }
   ],
   "source": [
    "test_image_path = 'split_discs/131_t2_disc_4.png'\n",
    "\n",
    "# Load, resize, and preprocess the test image\n",
    "img = Image.open(test_image_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "img_array = np.array(img_resized)\n",
    "\n",
    "# Preprocess the image for ResNet50\n",
    "img_preprocessed = tf.keras.applications.resnet50.preprocess_input(img_array)\n",
    "img_preprocessed = np.expand_dims(img_preprocessed, axis=0)  # Add batch dimension\n",
    "\n",
    "# Run the prediction\n",
    "predicted_probabilities = model.predict(img_preprocessed)\n",
    "predicted_pfirrman = np.argmax(predicted_probabilities[0]) + 1  # Add 1 if classes are 1-5\n",
    "predicted_modic = np.argmax(predicted_probabilities[1])\n",
    "print(f\"Predicted Pfirrman grade: {predicted_pfirrman}\")\n",
    "print(f\"Predicted Modic: {predicted_modic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4953efc6-9260-4485-9c04-f6b080ccbdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Image name: 131_t2_disc_4.png\n",
      "  Pfirrman Grade: 1\n",
      "  Modic Type: 0\n",
      "  UP Endplate: 0\n",
      "  LOW Endplate: 0\n",
      "  Spondylolisthesis: 0\n",
      "  Disc Herniation: 0\n",
      "  Disc Narrowing: 0\n",
      "  Disc Bulging: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 164\n",
    "print(f\"  Image name: {image_names[idx]}\")\n",
    "print(f\"  Pfirrman Grade: {pfirrman_grades[idx]}\")\n",
    "print(f\"  Modic Type: {modic_types[idx]}\")\n",
    "print(f\"  UP Endplate: {up_endplates[idx]}\")\n",
    "print(f\"  LOW Endplate: {low_endplates[idx]}\")\n",
    "print(f\"  Spondylolisthesis: {spondylolisthesis[idx]}\")\n",
    "print(f\"  Disc Herniation: {disc_herniation[idx]}\")\n",
    "print(f\"  Disc Narrowing: {disc_narrowing[idx]}\")\n",
    "print(f\"  Disc Bulging: {disc_bulging[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100abe2c-f1f4-46cb-820c-d6b92d8e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nonBinaryIndividualPredictions.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10bb34-2963-4b49-b2a3-211ca121f933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
